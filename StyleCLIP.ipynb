{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **StyleCLIP** 프로젝트\n",
    "\n",
    "**StyleCLIP**을 활용해 **사용자가 원하는 이미지**를 만들어내는 프로젝트\n",
    "\n",
    "필요한 모듈(ex.CLIP)은 사전에 install 한 뒤 사용(README.md 참조)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load the Pre-trained StyleGAN Model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())\n",
    "os.chdir('PyTorch-StyleGAN-Face-Editting')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "from torchvision import models \n",
    "from torchvision import transforms\n",
    "from torchvision.utils import save_image\n",
    "\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 이 파일외 GPU 사용 끊기\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stylegan_model import G_mapping\n",
    "from stylegan_model import G_synthesis\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "resolution = 1024\n",
    "weight_file = 'weights/karras2019stylegan-ffhq-1024x1024.pt'\n",
    "\n",
    "g_all = nn.Sequential(OrderedDict([\n",
    "    ('g_mapping', G_mapping()),\n",
    "    ('g_synthesis', G_synthesis(resolution=resolution))    \n",
    "]))\n",
    "g_all.load_state_dict(torch.load(weight_file, map_location=device))\n",
    "g_all.eval()\n",
    "g_all.to(device)\n",
    " \n",
    "g_mapping, g_synthesis = g_all[0], g_all[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show a torch image\n",
    "def imshow(tensor):\n",
    "    image = tensor.cpu().clone()\n",
    "    image = image.squeeze(0)\n",
    "    gray_scale = False\n",
    "    if image.shape[0] == 1:\n",
    "        gray_scale = True\n",
    "    image = transforms.ToPILImage()(image)\n",
    "    if gray_scale:\n",
    "        plt.imshow(image, cmap='gray')\n",
    "    else:\n",
    "        plt.imshow(image)\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Change the display resolution\n",
    "plt.rcParams['figure.figsize'] = [12, 8]\n",
    "plt.rcParams['figure.dpi'] = 80"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **사용할 latent vector 불러와서 확인**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 원본 이미지, 원본 이미지에 대응하는 latent vector에 해당하는 주소 반환\n",
    "def return_paths(img_name):\n",
    "    img_path = os.path.join(\"PyTorch-StyleGAN-Face-Editting\", \"images\", img_name+'.jpg')\n",
    "    latent_path = os.path.join(\"latent vectors\", img_name + '_latent.npy')\n",
    "\n",
    "    return img_path, latent_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths(\"엠마스톤\")\n",
    "latent_code_init = torch.tensor(np.load(latent_path)).to(device)\n",
    "latent_code_init.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    generated_image = g_synthesis(latent_code_init)\n",
    "    generated_image = (generated_image + 1.0) / 2.0\n",
    "    generated_image = generated_image.clamp(0, 1)\n",
    "    print(generated_image.shape)\n",
    "    imshow(generated_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CLIP Loss 정의**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import clip\n",
    "import torch.nn as nn\n",
    "\n",
    "class CLIPLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CLIPLoss, self).__init__()\n",
    "        self.model, self.preprocess = clip.load(\"ViT-B/32\")\n",
    "        self.upsample = torch.nn.Upsample(scale_factor=7)\n",
    "        self.avg_pool = torch.nn.AvgPool2d(kernel_size=32)\n",
    "    \n",
    "    def forward(self, image, text):\n",
    "        image = self.avg_pool(self.upsample(image)) # (1, 3, 224, 224)\n",
    "        similarity = 1 - self.model(image, text)[0] / 100\n",
    "        return  similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Latent Optimization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "\n",
    "\n",
    "args = Namespace()\n",
    "args.description = 'really sad face' \n",
    "args.lr_rampup = 0.05  \n",
    "args.lr = 0.1          # learning rate\n",
    "args.step = 150        # Updata step  \n",
    "args.l2_lambda = 0.005 # The weight for similarity to the original image.\n",
    "args.save_intermediate_image_every = 10\n",
    "args.results_dir = 'StyleCLIP_results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import torchvision\n",
    "from torch import optim\n",
    "\n",
    "\n",
    "# The learning rate adjustment function.\n",
    "def get_lr(t, initial_lr, rampdown=0.50, rampup=0.05):\n",
    "    lr_ramp = min(1, (1 - t) / rampdown)\n",
    "    lr_ramp = 0.5 - 0.5 * math.cos(lr_ramp * math.pi)\n",
    "    lr_ramp = lr_ramp * min(1, t / rampup)\n",
    "\n",
    "    return initial_lr * lr_ramp\n",
    "\n",
    "text_inputs = torch.cat([clip.tokenize(args.description)]).to(device) # clip에 들어가는 text 토큰화 -> text\n",
    "os.makedirs(args.results_dir, exist_ok=True)\n",
    "\n",
    "# Initialize the latent vector to be updated.\n",
    "latent = latent_code_init.detach().clone()\n",
    "latent.requires_grad = True\n",
    "\n",
    "clip_loss = CLIPLoss()\n",
    "optimizer = optim.Adam([latent], lr=args.lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(args.step):\n",
    "    # Adjust the learning rate.\n",
    "    t = i / args.step\n",
    "    lr = get_lr(t, args.lr) # step 마다 learning rate update\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "\n",
    "    # Generate an image using the latent vector.\n",
    "    img_gen = g_synthesis(latent)\n",
    "\n",
    "    # Calculate the loss value.\n",
    "    c_loss = clip_loss(img_gen, text_inputs)\n",
    "    l2_loss = ((latent_code_init - latent) ** 2).sum()\n",
    "    loss = c_loss + args.l2_lambda * l2_loss\n",
    "\n",
    "    # Get gradient and update the latent vector.\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # Log the current state.\n",
    "    print(f\"lr: {lr}, loss: {loss.item():.4f}\")\n",
    "    if args.save_intermediate_image_every > 0 and i % args.save_intermediate_image_every == 0:\n",
    "        with torch.no_grad():\n",
    "            img_gen = g_synthesis(latent)\n",
    "        torchvision.utils.save_image(img_gen, f\"results/{str(i).zfill(5)}.png\", normalize=True, range=(-1, 1))\n",
    "\n",
    "with torch.no_grad():\n",
    "    img_orig = g_synthesis([latent_code_init], input_is_latent=True, randomize_noise=False)\n",
    "\n",
    "# Display the initial image and result image.\n",
    "final_result = torch.cat([img_orig, img_gen])\n",
    "torchvision.utils.save_image(final_result.detach().cpu(), os.path.join(args.results_dir, \"final_result.jpg\"), normalize=True, scale_each=True, range=(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Visualization Result**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_image = ToPILImage()(make_grid(final_result.detach().cpu(), normalize=True, scale_each=True, range=(-1, 1), padding=0))\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "10385ddd246f550e7067f180f4f232d5cedb2ee0eaba8b0c683ba3b06d151ca8"
  },
  "kernelspec": {
   "display_name": "Python 3.7.11 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
