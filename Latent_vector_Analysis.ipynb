{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Latent vector Analysis**\n",
    "\n",
    "Image2Style을 통해 다양한 이미지를 latent vector로 embedding하는 실험을 진행했고, 결과를 분석하고자 한다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "import torchvision \n",
    "from torchvision import transforms\n",
    "from torchvision.utils import make_grid\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torchvision.utils import save_image\n",
    "from collections import OrderedDict\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('PyTorch-StyleGAN-Face-Editting')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stylegan_model import G_mapping\n",
    "from stylegan_model import G_synthesis\n",
    "\n",
    "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)\n",
    "resolution = 1024\n",
    "weight_file = 'weights/karras2019stylegan-ffhq-1024x1024.pt'\n",
    "\n",
    "g_all = nn.Sequential(OrderedDict([\n",
    "    ('g_mapping', G_mapping()),\n",
    "    ('g_synthesis', G_synthesis(resolution=resolution))    \n",
    "]))\n",
    "g_all.load_state_dict(torch.load(weight_file, map_location=device))\n",
    "g_all.eval()\n",
    "g_all.to(device)\n",
    " \n",
    "g_mapping, g_synthesis = g_all[0], g_all[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir('../')\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read an image from a file\n",
    "def image_reader(image_path, resize=None):\n",
    "    with open(image_path, \"rb\") as f:\n",
    "        image = Image.open(f)\n",
    "        image = image.convert(\"RGB\")\n",
    "    if resize != None:\n",
    "        image = image.resize((resize, resize))\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor() # [0, 1]\n",
    "    ])\n",
    "    image = transform(image)\n",
    "    image = image.unsqueeze(0) # (N, C, H, W)\n",
    "    return image\n",
    "\n",
    "# 원본이미지와 embedding을 통헤 구한 latent vector를 통해 만들어낸 이미지를 나란히 그리드로 만들어서 반환 \n",
    "def make_grid_function(img_path, latent_path):\n",
    "    img_orig = image_reader(img_path, resize=1024)\n",
    "    latent_code = torch.tensor(np.load(latent_path)).to(device)\n",
    "    generated_image = g_synthesis(latent_code)\n",
    "    generated_image = (generated_image + 1.0) / 2.0\n",
    "    generated_image = generated_image.clamp(0, 1)\n",
    "    final_result = torch.cat([img_orig, generated_image.detach().cpu()])\n",
    "    result_image = ToPILImage()(make_grid(final_result, normalize=True, scale_each=True, range=(0, 1), padding=0))\n",
    "    \n",
    "    return result_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_names = ['고윤정', '공유', '디카프리오', '수지', '아이유', '엠마스톤', '오바마_1', '오바마_2', '제니', '로제', '푸린', '호날두', '휴잭맨', 'cat', '조커', 'tiger_cartoon', 'Jablonski']\n",
    "\n",
    "# 원본 이미지, 원본 이미지에 대응하는 latent vector에 해당하는 주소 반환\n",
    "def return_paths(img_name):\n",
    "    img_path = os.path.join(\"PyTorch-StyleGAN-Face-Editting\", \"images\", img_name+'.jpg')\n",
    "    latent_path = os.path.join(\"latent vectors\", img_name + '_latent.npy')\n",
    "\n",
    "    return img_path, latent_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 테스트\n",
    "img_path, latent_path = return_paths('고유정')\n",
    "print(img_path)\n",
    "print(latent_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Good case**\n",
    "\n",
    "결과가 **좋은 케이스**들은 대체적으로 다음과 같은 특징을 가지고 있다는 것을 실험적으로 확인했다.\n",
    "\n",
    "1. 서양인임. \n",
    "2. input으로 들어가는 이미지에 noise가 거의 없음, 화질 좋음.\n",
    "3. 사진에 얼굴이 차지하는 비율이 거의 80퍼~90퍼(얼굴외에 손이나 기타 다른 부위 혹은 배경들이 거의 보이지 않음)\n",
    "5. 옆모습이나 측면보다 정면샷으로 찍은 얼굴이 더 잘 나옴.\n",
    "\n",
    "결과가 좋은 케이스-> 엠마스톤, 오바마1, 휴잭맨, celeba1, 제니, cat, 조커 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('엠마스톤')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('오바마1')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('Jablonski')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('제니')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('로제')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('조커')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **BAD case**\n",
    "\n",
    "결과가 **나쁜 케이스**들은 대체적으로 다음과 같은 특징을 가지고 있다는 것을 실험적으로 확인했다.\n",
    "\n",
    "1. 동양인이 대체로 많음. -> StyleGAN 자체가 FFHQ로 Pretrain되었는데, 동양인의 특징을 잘 반영할 수 있는 이미지들이 부족하지 않았나 생각..(특히 눈)\n",
    "2. input으로 들어가는 이미지에 noise가 많음, 화질 안 좋음.\n",
    "3. 사진에 얼굴 외에도 다른 부가적인 요소들이 많음. (EX. 화려한 배경, 옷)\n",
    "4. 얼굴 중 특히 눈에 대해서 자연스러운 이미지 생성 X\n",
    "5. 포켓몬스터 -> 1.과 마찬가지로 pretrain시 한번도 보지 않았을 이미지일 확률 높음.\n",
    "\n",
    "결과가 안 좋은 케이스-> 고윤정, 공유, 수지, 아이유, 오바마_2, 푸린"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('고윤정')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('수지')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('공유')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('호날두')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path, latent_path = return_paths('푸린')\n",
    "result_image = make_grid_function(img_path, latent_path)\n",
    "h, w = result_image.size\n",
    "result_image.resize((h // 2, w // 2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Latent Interpolation**\n",
    "\n",
    "앞서 결과가 잘 나오는 latent vector를 통해, latent Interpolation 진행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rose to Emma Stone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path1, latent_path1 = return_paths('로제')\n",
    "img_path2, latent_path2 = return_paths('엠마스톤')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent1 = torch.tensor(np.load(latent_path1)).to(device) # 로제 \n",
    "latent2 = torch.tensor(np.load(latent_path2)).to(device) # 엠마스톤"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Interpolation Experiment\n",
    "num_imgs = 7 # 변화 단계 설정\n",
    "for i in range(num_imgs):\n",
    "  a = (1/num_imgs)*i\n",
    "  w = latent1 * (1-a)+ latent2 * a\n",
    "  w = w.to(device)\n",
    "  syn_img = g_synthesis(w)\n",
    "  syn_img = (syn_img+1.0)/2.0\n",
    "  save_image(syn_img.clamp(0,1),\"latent Interpolation/Rose_to_Emma/Rose_to_Emma{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_imgs_dir = os.listdir('latent Interpolation/Rose_to_Emma')\n",
    "inter_imgs = torch.tensor([])\n",
    "for idx in range(num_imgs):\n",
    "    total_path = os.path.join('latent Interpolation/Rose_to_Emma','Rose_to_Emma'+str(idx)+'.png')\n",
    "    img = image_reader(total_path, resize=1024)\n",
    "    inter_imgs = torch.cat([inter_imgs, img], dim=0)\n",
    "\n",
    "print(inter_imgs.shape)\n",
    "result_image = ToPILImage()(make_grid(inter_imgs))\n",
    "result_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rose to Jennie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path1, latent_path1 = return_paths('로제')\n",
    "img_path2, latent_path2 = return_paths('제니')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent1 = torch.tensor(np.load(latent_path1)).to(device) # 로제 \n",
    "latent2 = torch.tensor(np.load(latent_path2)).to(device) # 제니"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Interpolation Experiment\n",
    "num_imgs = 7\n",
    "for i in range(num_imgs):\n",
    "  a = (1/num_imgs)*i\n",
    "  w = latent1 * (1-a)+ latent2 * a\n",
    "  w = w.to(device)\n",
    "  syn_img = g_synthesis(w)\n",
    "  syn_img = (syn_img+1.0)/2.0\n",
    "  save_image(syn_img.clamp(0,1),\"latent Interpolation/Rose_to_Jennie/Rose_to_Jennie{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_imgs = torch.tensor([])\n",
    "for idx in range(num_imgs):\n",
    "    total_path = os.path.join('latent Interpolation/Rose_to_Jennie','Rose_to_Jennie'+str(idx)+'.png')\n",
    "    img = image_reader(total_path, resize=1024)\n",
    "    inter_imgs = torch.cat([inter_imgs, img], dim=0)\n",
    "\n",
    "print(inter_imgs.shape)\n",
    "result_image = ToPILImage()(make_grid(inter_imgs))\n",
    "result_image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Obama to Joker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_path1, latent_path1 = return_paths('오바마1')\n",
    "img_path2, latent_path2 = return_paths('조커')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent1 = torch.tensor(np.load(latent_path1)).to(device) # 오바마\n",
    "latent2 = torch.tensor(np.load(latent_path2)).to(device) # 조커"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Image Interpolation Experiment\n",
    "num_imgs = 7\n",
    "for i in range(num_imgs):\n",
    "  a = (1/num_imgs)*i\n",
    "  w = latent1 * (1-a)+ latent2 * a\n",
    "  w = w.to(device)\n",
    "  syn_img = g_synthesis(w)\n",
    "  syn_img = (syn_img+1.0)/2.0\n",
    "  save_image(syn_img.clamp(0,1),\"latent Interpolation/Obama_to_Joker/Obama_to_Joker{}.png\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inter_imgs = torch.tensor([])\n",
    "for idx in range(num_imgs):\n",
    "    total_path = os.path.join('latent Interpolation/Obama_to_Joker','Obama_to_Joker'+str(idx)+'.png')\n",
    "    img = image_reader(total_path, resize=1024)\n",
    "    inter_imgs = torch.cat([inter_imgs, img], dim=0)\n",
    "\n",
    "print(inter_imgs.shape)\n",
    "result_image = ToPILImage()(make_grid(inter_imgs))\n",
    "result_image"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "6fa8c4a0213b3e8e46e64ca221d4ef2f7254b1e53b83d6209b624a99d7aa7db4"
  },
  "kernelspec": {
   "display_name": "Python 3.8.5 32-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
